exclude_nonstate_attribs <- -match(c("Military", "Other US"), uiuc_statedata_2000$State)
uiuc_statedata_2000 <- uiuc_statedata_2000[exclude_nonstate_attribs, ]
popTotalUS2000 <- sum(popUS2000$Pop)
popUGUIUC2000 <- sum(uiuc_statedata_2000$Undergrad)
true_US_props = popUS2000$Pop / popTotalUS2000
# perform goodness-of-fit test on original data
test_result_orig = chisq.test(uiuc_statedata_2000$Undergrad, p = true_US_props)
p_val_orig = test_result_orig$p.value
chisq_stat_orig = test_result_orig$statistic
print(chisq_stat_orig)
p_val_orig
N_perms = 1000
# get p-value for chi-square test for each permutation
p_vals = numeric(N_perms)
chisq_stat_vals = numeric(N_perms)
uiuc_UGstatepop_perm = uiuc_statedata_2000$Undergrad
for (i in 1:N_perms) {
# shuffle the population for each state
uiuc_UGstatepop_perm = sample(uiuc_UGstatepop_perm)
# run the chi-square test, store the p-value
test_results = chisq.test(uiuc_UGstatepop_perm, p = true_US_props)
p_vals[i] = test_results$p.value
chisq_stat_vals[i] = test_results$statistic
}
# plot the distribution
hist(p_vals)
hist(chisq_stat_vals)
# get the proportion of "permuted p-values" less than the original; this is the ASL
alpha = 0.01
asl = ( 1 + sum(chisq_stat_vals >= chisq_stat_orig) ) / (1 + N_perms)
print(asl)
asl < alpha
proximal_states = c('Illinois', 'Indiana', 'Wisconsin', 'Iowa', 'Missouri', 'Kentucky')
exclude_proximal_states = -match(proximal_states, uiuc_statedata_2000$State)
uiuc_nonproximal_2000 = uiuc_statedata_2000[exclude_proximal_states, ]
us_nonproximal_2000 = popUS2000[exclude_proximal_states, ]
popNonproximalUS2000 = sum(us_nonproximal_2000$Pop)
popNonproximalUGUIUC2000 = sum(uiuc_nonproximal_2000$Undergrad)
true_nonproximal_US_props = us_nonproximal_2000$Pop / popNonproximalUS2000
test_result_nonprox_orig = chisq.test(uiuc_nonproximal_2000$Undergrad,
p = true_nonproximal_US_props)
chisq_stat_nonprox_orig = test_result_nonprox_orig$statistic
p_val_nonprox_orig = test_result_nonprox_orig$p.value
print(chisq_stat_nonprox_orig)
p_val_nonprox_orig
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
state_data <- read.csv("uiuc_students_by_state.csv", header = TRUE)
popUS2000 <- read.csv("state_2000.csv", header = FALSE)
colnames(popUS2000) = c("State", "Pop")
# Sort the population data in alphabetical order to match the UIUC data
popUS2000 <- popUS2000[order(popUS2000$State), ]
uiuc_statedata_2000 <- state_data[state_data$Year == 2000, ]
# Exclude non-states that don't appear in our US population data
exclude_nonstate_attribs <- -match(c("Military", "Other US"), uiuc_statedata_2000$State)
uiuc_statedata_2000 <- uiuc_statedata_2000[exclude_nonstate_attribs, ]
popTotalUS2000 <- sum(popUS2000$Pop)
popUGUIUC2000 <- sum(uiuc_statedata_2000$Undergrad)
true_US_props = popUS2000$Pop / popTotalUS2000
# perform goodness-of-fit test on original data
test_result_orig = chisq.test(uiuc_statedata_2000$Undergrad, p = true_US_props)
p_val_orig = test_result_orig$p.value
chisq_stat_orig = test_result_orig$statistic
print(chisq_stat_orig)
p_val_orig
N_perms = 1000
# get p-value for chi-square test for each permutation
p_vals = numeric(N_perms)
chisq_stat_vals = numeric(N_perms)
uiuc_UGstatepop_perm = uiuc_statedata_2000$Undergrad
for (i in 1:N_perms) {
# shuffle the population for each state
uiuc_UGstatepop_perm = sample(uiuc_UGstatepop_perm)
# run the chi-square test, store the p-value
test_results = chisq.test(uiuc_UGstatepop_perm, p = true_US_props)
p_vals[i] = test_results$p.value
chisq_stat_vals[i] = test_results$statistic
}
# plot the distribution
hist(p_vals)
hist(chisq_stat_vals)
# get the proportion of "permuted p-values" less than the original; this is the ASL
alpha = 0.01
asl = ( 1 + sum(chisq_stat_vals >= chisq_stat_orig) ) / (1 + N_perms)
print(asl)
asl < alpha
proximal_states = c('Illinois', 'Indiana', 'Wisconsin', 'Iowa', 'Missouri', 'Kentucky')
exclude_proximal_states = -match(proximal_states, uiuc_statedata_2000$State)
uiuc_nonproximal_2000 = uiuc_statedata_2000[exclude_proximal_states, ]
us_nonproximal_2000 = popUS2000[exclude_proximal_states, ]
popNonproximalUS2000 = sum(us_nonproximal_2000$Pop)
popNonproximalUGUIUC2000 = sum(uiuc_nonproximal_2000$Undergrad)
true_nonproximal_US_props = us_nonproximal_2000$Pop / popNonproximalUS2000
test_result_nonprox_orig = chisq.test(uiuc_nonproximal_2000$Undergrad,
p = true_nonproximal_US_props)
chisq_stat_nonprox_orig = test_result_nonprox_orig$statistic
p_val_nonprox_orig = test_result_nonprox_orig$p.value
print(chisq_stat_nonprox_orig)
p_val_nonprox_orig
chisq_stats_nonprox = numeric(N_perms)
uiuc_nonproximal2000_perm = uiuc_nonproximal_2000$Undergrad
for (i in 1:N_perms) {
# cells contain for UIUC the number of people from each state
# cells for US contain proportion of US population from each state
uiuc_nonproximal2000_perm = sample(uiuc_nonproximal2000_perm)
# run the chi-square test, store the p-value
chisq_stats_nonprox[i] = chisq.test(uiuc_nonproximal2000_perm, p = true_nonproximal_US_props)$statistic
}
# plot the distribution
hist(chisq_stats_nonprox)
asl_nonprox = ( 1 + sum(chisq_stats_nonprox >= chisq_stat_nonprox_orig) ) / (1 + N_perms)
print(asl_nonprox)
asl_nonprox < alpha
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
state_data <- read.csv("uiuc_students_by_state.csv", header = TRUE)
popUS2000 <- read.csv("state_2000.csv", header = FALSE)
colnames(popUS2000) = c("State", "Pop")
# Sort the population data in alphabetical order to match the UIUC data
popUS2000 <- popUS2000[order(popUS2000$State), ]
uiuc_statedata_2000 <- state_data[state_data$Year == 2000, ]
# Exclude non-states that don't appear in our US population data
exclude_nonstate_attribs <- -match(c("Military", "Other US"), uiuc_statedata_2000$State)
uiuc_statedata_2000 <- uiuc_statedata_2000[exclude_nonstate_attribs, ]
popTotalUS2000 <- sum(popUS2000$Pop)
popUGUIUC2000 <- sum(uiuc_statedata_2000$Undergrad)
true_US_props = popUS2000$Pop / popTotalUS2000
# perform goodness-of-fit test on original data
test_result_orig = chisq.test(uiuc_statedata_2000$Undergrad, p = true_US_props)
p_val_orig = test_result_orig$p.value
chisq_stat_orig = test_result_orig$statistic
print(chisq_stat_orig)
p_val_orig
# perform goodness-of-fit test on original data
test_result_orig = chisq.test(uiuc_statedata_2000$Undergrad, p = true_US_props)
test_result_orig
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
state_data <- read.csv("uiuc_students_by_state.csv", header = TRUE)
popUS2000 <- read.csv("state_2000.csv", header = FALSE)
colnames(popUS2000) = c("State", "Pop")
# Sort the population data in alphabetical order to match the UIUC data
popUS2000 <- popUS2000[order(popUS2000$State), ]
uiuc_statedata_2000 <- state_data[state_data$Year == 2000, ]
# Exclude non-states that don't appear in our US population data
exclude_nonstate_attribs <- -match(c("Military", "Other US"), uiuc_statedata_2000$State)
uiuc_statedata_2000 <- uiuc_statedata_2000[exclude_nonstate_attribs, ]
popTotalUS2000 <- sum(popUS2000$Pop)
popUGUIUC2000 <- sum(uiuc_statedata_2000$Undergrad)
true_US_props = popUS2000$Pop / popTotalUS2000
# perform goodness-of-fit test on original data
test_result_orig = chisq.test(uiuc_statedata_2000$Undergrad, p = true_US_props)
p_val_orig = test_result_orig$p.value
chisq_stat_orig = test_result_orig$statistic
print(chisq_stat_orig)
p_val_orig
N_perms = 1000
# get p-value for chi-square test for each permutation
p_vals = numeric(N_perms)
chisq_stat_vals = numeric(N_perms)
uiuc_UGstatepop_perm = uiuc_statedata_2000$Undergrad
for (i in 1:N_perms) {
# shuffle the population for each state
uiuc_UGstatepop_perm = sample(uiuc_UGstatepop_perm)
# run the chi-square test, store the p-value
test_results = chisq.test(uiuc_UGstatepop_perm, p = true_US_props)
p_vals[i] = test_results$p.value
chisq_stat_vals[i] = test_results$statistic
}
# plot the distribution
hist(p_vals)
hist(chisq_stat_vals)
# get the proportion of "permuted statistics" greater than the original; this is the ASL
alpha = 0.01
asl = ( 1 + sum(chisq_stat_vals >= chisq_stat_orig) ) / (1 + N_perms)
print(asl)
asl < alpha
proximal_states = c('Illinois', 'Indiana', 'Wisconsin', 'Iowa', 'Missouri', 'Kentucky')
exclude_proximal_states = -match(proximal_states, uiuc_statedata_2000$State)
uiuc_nonproximal_2000 = uiuc_statedata_2000[exclude_proximal_states, ]
us_nonproximal_2000 = popUS2000[exclude_proximal_states, ]
popNonproximalUS2000 = sum(us_nonproximal_2000$Pop)
popNonproximalUGUIUC2000 = sum(uiuc_nonproximal_2000$Undergrad)
true_nonproximal_US_props = us_nonproximal_2000$Pop / popNonproximalUS2000
test_result_nonprox_orig = chisq.test(uiuc_nonproximal_2000$Undergrad,
p = true_nonproximal_US_props)
chisq_stat_nonprox_orig = test_result_nonprox_orig$statistic
p_val_nonprox_orig = test_result_nonprox_orig$p.value
print(chisq_stat_nonprox_orig)
p_val_nonprox_orig
test_result_nonprox_orig
state_data <- read.csv("uiuc_students_by_state.csv", header = TRUE)
popUS2000 <- read.csv("state_2000.csv", header = FALSE)
View(state_data)
rm(list=ls())
state_data <- read.csv("uiuc_students_by_state.csv", header = TRUE)
popUS2000 <- read.csv("state_2000.csv", header = FALSE)
View(popUS2000)
?new_site
?blogdown::new_site
blogdown::new_site(theme = "nodejh/hugo-theme-cactus-plus")
library(blogdown)
serve_site()
blogdown::serve_site()
?serve_site
setwd("~/website")
blogdown::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
install.packages("httpuv")
install.packages("httpuv")
blogdown:::new_post_addin()
blogdown::serve_site()
blogdown::serve_site()
blogdown::hugo_version()
blogdown::serve_site()
?blogdown::serve_site
blogdown::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
traceback()
blogdown::build_site()
blogdown:::serve_site()
file.remove(list.files(, '[.]Rmd$', recursive = TRUE, full.names = TRUE))
blogdown:::serve_site()
install.packages("later")
install.packages("later")
install.packages("httpuv")
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown::stop_server()
sessionInfo()
install.packages(servr)
install.packages("servr")
install.packages("servr")
blogdown:::serve_site()
library(blogdown)
install.packages("blogdown")
blogdown::serve_site()
blogdown::new_site(theme = 'gcushen/academic')
?blogdown::new_site()
blogdown::serve_site()
blogdown::install_theme("gcushen/hugo-academic")
blogdown:::serve_site()
blogdown::hugo_build()
?blogdown::install_theme()
blogdown::install_theme("gcushen/hugo-academic")
blogdown::update_hugo()
blogdown::hugo_version()
blogdown:::find_hugo()
blogdown:::serve_site()
blogdown::hugo_version()
blogdown::serve_site()
servr::daemon_stop(1)
install.packages(c("spacyr", "tidyverse"))
library(broom)
install.packages("broom")
knitr::opts_chunk$set(echo = TRUE)
library(spacyr)
library(tidyverse)
spacy_initialize(model = "en_core_web_sm")
fps_lyrics_tif = read_csv("data/female_pop_star_lyrics.csv") %>%
tibble::rowid_to_column() %>%
mutate(doc_id = rowid, text = lyric)
spacy_extract_entity(fps_lyrics_tif)
install.packages("tif")
devtools::install_github("ropensci/tif")
install.packages("devtools")
devtools::install_github("ropensci/tif")
library(tif)
tif_is_corpus_df(fps_lyrics_tif)
fps_lyrics_tif
?tif::tif_as_corpus_fd
?tif::tif_as_corpus_df
fps_lyrics_tif %>% names()
fps_lyrics_tif
?%<>%
%<>%
1 %<>% 2
?muutate_at
?mutate_at
fps_lyrics_tif = read_csv("data/female_pop_star_lyrics.csv") %>%
tibble::rowid_to_column() %>%
mutate(doc_id = rowid, text = lyric) %>%
mutate_at(doc_id, as.character)
fps_lyrics_tif = read_csv("data/female_pop_star_lyrics.csv") %>%
tibble::rowid_to_column() %>%
mutate(doc_id = rowid, text = lyric) %>%
mutate_at(doc_id, as.character)
# use_python("/Users/carsonzhang/anaconda3/bin/python")
# spacy_uninstall()
# spacy_install()
# remove.packages("Rcpp")
# remove.packages("reticulate")
# install.packages("Rcpp")
# install.packages("reticulate")
# spacy_uninstall()
spacy_initialize(model = "en_core_web_sm")
fps_lyrics_tif = read_csv("data/female_pop_star_lyrics.csv") %>%
tibble::rowid_to_column() %>%
mutate(doc_id = rowid, text = lyric) %>%
mutate(doc_id = as.character(doc_id))
# write_csv(fps_lyrics_tif, "data/fps_lyrics_tif.csv")
# py_install("spaCy")
library(tif)
tif_is_corpus_df(fps_lyrics_tif)
spacy_extract_entity(fps_lyrics_tif)
fps_lyrics_tif = read_csv("data/female_pop_star_lyrics.csv") %>%
tibble::rowid_to_column() %>%
mutate(doc_id = rowid, text = lyric) %>%
mutate(doc_id = as.character(doc_id))
# use_python("/Users/carsonzhang/anaconda3/bin/python")
# spacy_uninstall()
# spacy_install()
# remove.packages("Rcpp")
# remove.packages("reticulate")
# install.packages("Rcpp")
# install.packages("reticulate")
# spacy_uninstall()
spacy_initialize(model = "en_core_web_sm")
fps_lyrics_tif = read_csv("data/female_pop_star_lyrics.csv") %>%
tibble::rowid_to_column() %>%
mutate(doc_id = rowid, text = lyric) %>%
mutate(doc_id = as.character(doc_id))
# write_csv(fps_lyrics_tif, "data/fps_lyrics_tif.csv")
# py_install("spaCy")
library(tif)
tif_is_corpus_df(fps_lyrics_tif)
spacy_extract_entity(fps_lyrics_tif)
library(tif)
tif_is_corpus_df(fps_lyrics_tif)
fps_lyrics_tif = read_csv("data/female_pop_star_lyrics.csv") %>%
tibble::rowid_to_column() %>%
mutate(doc_id = rowid, text = lyric) %>%
mutate(doc_id = as.character(doc_id)) %>%
top_n(5)
fps_lyrics_tif
?top_n
fps_lyrics_tif = read_csv("data/female_pop_star_lyrics.csv") %>%
tibble::rowid_to_column() %>%
mutate(doc_id = rowid, text = lyric) %>%
mutate(doc_id = as.character(doc_id)) %>%
head()
fps_lyrics_tif
?head
fps_lyrics_tif
library(tif)
tif_is_corpus_df(fps_lyrics_tif)
spacy_extract_entity(fps_lyrics_tif)
fps_parse = spacy_parse(fps_lyrics_tif)
fps_parse
fps_lyrics_tif = read_csv("data/female_pop_star_lyrics.csv") %>%
tibble::rowid_to_column() %>%
mutate(doc_id = rowid, text = lyric) %>%
mutate(doc_id = as.character(doc_id))
fps_parse = spacy_parse(fps_lyrics_tif)
fps_np = nounphrase_extract(fps_lyrics_tif)
?nounphrase_extract
fps_parsed = spacy_parse(fps_lyrics_tif)
fps_parsed
fps_parsed %>%
filter(pos == "PRON")
fps_parsed
nounphrase_extract(fps_parsed)
?spacy_parse
fps_parsed
fps_parsed %>% filter(pos == "PRON")
fps_parsed %>% filter(pos == "PRON")
fps_parsed %>% filter(pos == "PRON") %>% filter(doc_id == 1)
fps_parsed %>% filter(pos == "PRON") %>% filter(doc_id == 1) -> first
rm(list = "first")
fps_parsed %>% filter(pos == "PRON") %>% filter(doc_id == 1) -> firstrow
firstrow
firstrow[, "entity"]
firstrow[, "pos"]
firstrow %>% select(entity)
fps_parsed %>% filter(pos == "PRON") %>% filter(entity != "")
fps_parsed %>% filter(pos == "PRON") %>% filter(entity != "") %>% nrow()
fps_parsed %>% filter(pos == "PRON") %>% nrow()
dj = fps_lyrics_tif %>%
filter(track_title == "Dear John")
dj
dj
?summarise
dj  %>%
summarise(whole_song = paste(lyric))
dj %>%
group_by(track_title) %>%
summarise(whole_song = paste(lyric))
dj %>%
summarise(whole_song = paste(lyric))
dj
?paste
dj %>%
summarise(whole_song = paste(lyric))
dj %>%
summarise(whole_song = paste(lyric, collapse = ''))
dj %>%
summarise(whole_song = paste(lyric, collapse = ', '))
dj %>%
summarise(whole_song = paste(lyric, collapse = '. '))
?paste
ts_lyrics = fps_lyrics_tif %>%
filter(artist == "Taylor Swift")
ts_lyrics
ts_lyrics %>%
group_by(track_title) %>%
summarise(whole_song = paste(lyric, collapse = '. '))
ts_lyrics %>%
group_by(track_title, artist) %>%
summarise(whole_song = paste(lyric, collapse = '. '))
ts_lyrics
ts_lyrics %>%
group_by(track_title, artist, album, track_n) %>%
summarise(whole_song = paste(lyric, collapse = '. '))
ts_lyrics
ts_lyrics %>%
group_by(track_title, artist, album, track_n) %>%
summarise(whole_song = paste(lyric, collapse = '. '))
ts_lyrics %>%
group_by(album, track_n, track_title, artist) %>%
summarise(whole_song = paste(lyric, collapse = '. '))
ts_lyrics %>%
group_by(album, track_n, track_title, artist) %>%
summarise(whole_song = paste(lyric, collapse = ' '))
ts_lyrics
ts_lyrics %>%
group_by(album, track_n, track_title, artist) %>%
summarise(text = paste(text, collapse = ' '))
# good! need to convert to TIF
# decide how to separate
ts_lyrics %>%
group_by(album, track_n, track_title, artist) %>%
summarise(whole_song = paste(lyric, collapse = ' ')) -> ts_songs
ts_parsed = spacy_parse(ts_songs)
ts_songs
# good! need to convert to TIF
# decide how to separate
ts_songs = ts_lyrics %>%
group_by(album, track_n, track_title, artist) %>%
summarise(whole_song = paste(lyric, collapse = ' '))
ts_songs
?mutuate
?mutate
ts_songs_tif = ts_songs %>%
mutate(doc_id = as.character(row_number()), text = whole_song)
ts_songs_tif
ts_songs_tif = ts_songs %>%
mutate(doc_id = row_number(), text = whole_song)
ts_songs_tif
?row_number
row_number
ts_songs %>% row_number()
row_number(ts_songs %>% select(artist))
ts_songs_tif = ts_songs %>%
mutate(doc_id = row_number(), text = whole_song)
ts_songs_tif
ts_songs
?seq.int
ts_songs_tif = ts_songs %>%
mutate(doc_id = row_number(artist), text = whole_song)
ts_songs_tif
ts_songs_tif = ts_songs %>%
mutate(doc_id = row_number(track_title), text = whole_song)
ts_songs_tif
# good! need to convert to TIF
# decide how to separate
ts_songs = ts_lyrics %>%
group_by(album, track_n, track_title, artist) %>%
summarise(whole_song = paste(lyric, collapse = ' '))
ts_songs
ts_songs_tif = ts_songs %>%
mutate(doc_id = row_number(track_title), text = whole_song)
ts_songs_tif
fps_lyrics_tif
ts_songs_tif = ts_songs %>%
tibble::rowid_to_column() %>%
# mutate(doc_id = as.character, text = whole_song)
ts_songs_tif
ts_songs_tif
?tibble::rowid_to_column
ts_songs_tif = ts_songs %>%
has_rownames()
ts_songs %>%
has_rownames()
ts_songs_tif = ts_songs %>%
rowid_to_column()
ts_songs_tif
ts_songs_tif = ts_songs %>%
rowid_to_column() %>%
mutate(doc_id = as.character(rowid), text = whole_song)
ts_songs_tif
ts_parsed = spacy_parse(ts_songs_tif)
ts_parsed
ts_parsed = spacy_parse(ts_songs_tif, tag = TRUE, entity = TRUE,
lemma = FALSE, nounphrase = TRUE)
ts_parsed
ts_np = spacy_extract_nounphrases(ts_parsed)
ts_parsed
ts_np = spacy_extract_nounphrases(ts_songs_tif)
ts_np
spacy_finalize()
?spacyr_initialize
?spacy_initialize
write_csv(fps_lyrics_tif, "data/fps_lyrics_tif.csv")
